{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ed023e",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Disaster Tweets\n",
    "\n",
    "## Problem Description\n",
    "The goal of this project is to classify tweets as either related to real disasters or not. This is a binary classification task where we analyze short texts using Natural Language Processing (NLP) techniques. The dataset consists of tweets labeled as either a real disaster (1) or not (0). We will use machine learning to automate this classification.\n",
    "\n",
    "## Dataset Overview\n",
    "- `id`: A unique identifier for each tweet.\n",
    "- `keyword`: A keyword from the tweet.\n",
    "- `location`: The location of the tweet.\n",
    "- `text`: The content of the tweet.\n",
    "- `target`: The label (1 for disaster-related, 0 for not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa7eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "import keras_nlp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "# Set TensorFlow Backend\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbfc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "df_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "\n",
    "# Data Overview\n",
    "print(f'Training Set Shape: {df_train.shape}')\n",
    "print(f'Test Set Shape: {df_test.shape}')\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b4216",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb2688",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"length\"] = df_train[\"text\"].apply(len)\n",
    "df_test[\"length\"] = df_test[\"text\"].apply(len)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df_train[\"length\"], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Tweet Lengths in Training Set\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5813701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.2\n",
    "EPOCHS = 3  # Slightly increased epochs for better performance\n",
    "\n",
    "# Train-Validation Split\n",
    "X = df_train[\"text\"]\n",
    "y = df_train[\"target\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n",
    "X_test = df_test[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ab03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DistilBERT model from Keras NLP\n",
    "preset = \"distil_bert_base_en_uncased\"\n",
    "preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset, sequence_length=160)\n",
    "classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset, preprocessor=preprocessor, num_classes=2)\n",
    "\n",
    "classifier.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "classifier.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33870fd3",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "history = classifier.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852200ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Display Confusion Matrix\n",
    "def display_confusion_matrix(y_true, y_pred, dataset):\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true, np.argmax(y_pred, axis=1),\n",
    "        display_labels=[\"Not Disaster\", \"Disaster\"],\n",
    "        cmap=plt.cm.Blues\n",
    "    )\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, np.argmax(y_pred, axis=1)).ravel()\n",
    "    f1_score = tp / (tp + ((fn + fp) / 2))\n",
    "    disp.ax_.set_title(f\"Confusion Matrix on {dataset} Dataset -- F1 Score: {round(f1_score, 2)}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4dac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "y_pred_train = classifier.predict(X_train)\n",
    "display_confusion_matrix(y_train, y_pred_train, \"Training\")\n",
    "\n",
    "y_pred_val = classifier.predict(X_val)\n",
    "display_confusion_matrix(y_val, y_pred_val, \"Validation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e50a2",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "- The model achieves reasonable accuracy, but there are misclassifications as seen in the confusion matrix.\n",
    "- The F1-score provides a better measure of performance, considering both precision and recall.\n",
    "- More advanced techniques such as fine-tuning the model, increasing epochs, or using additional feature engineering could improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Submission\n",
    "sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "sample_submission[\"target\"] = np.argmax(classifier.predict(X_test), axis=1)\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Submission file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69561389",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- This project demonstrated how NLP techniques can classify tweets related to disasters.\n",
    "- Using DistilBERT, we built and trained a model that can differentiate between disaster and non-disaster tweets.\n",
    "- Future improvements could involve hyperparameter tuning, ensemble models, or additional data sources to improve robustness."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
